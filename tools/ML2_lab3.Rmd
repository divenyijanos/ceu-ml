---
title: "CEU Machine Learning Tools - Session 3"
author: János Divényi
output: html_notebook
---

```{r}
library(tidyverse)
library(h2o)
h2o.init()
```


## Classify fashion images

The Fashion-MNIST dataset consists of Zalando’s article images. Each example is a 28×28 grayscale image, associated with a label from 10 classes:

1. T-shirt/top
2. Trouser
3. Pullover
4. Dress
5. Coat
6. Sandal
7. Shirt
8. Sneaker
9. Bag
10. Ankle boot

```{r load-fmnist}
fmnist_data <- read_csv("../data/fashion/fashion-mnist_train.csv")
fmnist_data <- mutate(fmnist_data,
    label = as.factor(label),
    across(-label, ~./255)
)
```

```{r plot-fashion-data}
labels <- c("T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot")
xy_axis <- as_tibble(expand.grid(x = 1:28, y = 28:1))
plot_theme <- list(
    raster = geom_raster(hjust = 0, vjust = 0),
    gradient_fill = scale_fill_gradient(low = "white", high = "black", guide = "none"),
    theme = theme(
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        panel.background = element_blank(),
        panel.border = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_blank()
    )
)

showImages <- function(data, row_indices) {
    list_of_plots <- map(row_indices, ~{
        cbind(xy_axis, fill = t(data[.x, -1])) |>
            ggplot(aes(x, y, fill = fill)) +
            coord_fixed() +
            plot_theme +
            labs(title = labels[data[[.x, 1]]])
    })
    do.call(gridExtra::grid.arrange, list_of_plots)
}
showImages(fmnist_data, 1:12)
```
```{r create-h2o-frames}
my_seed <- 20220323
# I assign only 10% to training to shorten the training time
data_split <- h2o.splitFrame(as.h2o(fmnist_data), ratios = 0.1, seed = my_seed)
fmnist_train <- data_split[[1]]
fmnist_holdout <- data_split[[2]]
```
```{r logit-baseline}
fmnist_logit <- h2o.glm(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_logit",
    lambda = 0,
    seed = my_seed
)
```

```{r logit-performance}
# plot the confusion matrix and calculate the mean per class error for the logit
```

```{r simple-dl-on-fashion}
fmnist_dl_default <- h2o.deeplearning(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_dl_default",
    seed = my_seed  # only reproducible if single threaded
)
# 199,210 params = 784 (input features) * 200 (first layer nodes) + 200 (first layer biases) +
#                  + 200 * 200 + 200 (second layer) +
#                  + 200 * 10 + 10 (output)
```

```{r investigate-h2o-dl}
h2o.confusionMatrix(fmnist_dl_default, valid = TRUE)
h2o.mean_per_class_error(fmnist_dl_default, train = TRUE, valid = TRUE)
h2o.scoreHistory(fmnist_dl_default)
plot(fmnist_dl_default, metric = "classification_error")
```
## DL parameters

```{r look-at-dl-params}
params <- fmnist_dl_default@allparameters
str(params)
?h2o.deeplearning
```

### Training samples

* `epochs`: how many times will all training data points be used to adjust the model in the course of the optimization (defaults to 10). Note: early stopping is used by default so there is no guarantee that all epochs will be used.

* `mini_batch_size`: after how many training samples is the gradient update made (defaults to 1)

### Regularization

You have multiple ways to regularize in a neural network. One is *"dropout"*, that "approximates training a large number of neural networks with different architectures in parallel" [source](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/). Some of the nodes are randomly ignored or “dropped out” during training. Another method is to apply *penalty terms* as we did with penalized linear regressions. You can also control when the iterative optimization process should stop - with *early stopping* you can also prevent overfitting.

* `hidden_dropout_ratios`: with how large probability will neurons be left out of the model at a step (defaults to 0.5). Have to use "*WithDropout" activation to use dropout.

* `input_dropout_ratio`: drop some input features randomly (defaults to 0).

* `l1`, `l2`: weight on $L1$ (lasso), $L2$ (ridge) penalty terms

* early stopping options: `stopping_rounds` (defaults to 5), `stopping_metric` (defaults to “logloss” for classification and “deviance” for regression), `stopping_tolerance` (defaults to 0.001)

Training constantly tracks validation frame performance. Early stopping is enabled by default but can be tuned when to stop. This, again, is to prevent overfitting.
(If you don't supply a `validation_frame`, early stopping still works but based on metrics calculated from the training set, so it may not be as informative for out-of-sample performance.)

```{r adjusted-dl-on-fashion}
fmnist_dl_adjusted <- h2o.deeplearning(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_dl_adjusted",
    hidden = 32,
    mini_batch_size = 20,
    score_each_iteration = TRUE,
    seed = my_seed
)
```

```{r adjusted-dl-scoring}
h2o.confusionMatrix(fmnist_dl_adjusted, valid = TRUE)
h2o.mean_per_class_error(fmnist_dl_adjusted, train = TRUE, valid = TRUE)
plot(fmnist_dl_adjusted, metric = "classification_error")
```

```{r regularize-dl-on-fashion}
fmnist_dl_regularized <- h2o.deeplearning(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_dl_regularized",
    hidden = c(16, 16),
    mini_batch_size = 20,
    activation = "RectifierWithDropout",
    hidden_dropout_ratios = c(0.2, 0.2),
    epochs = 300,
    score_each_iteration = TRUE,
    seed = my_seed
)
h2o.mean_per_class_error(fmnist_dl_regularized, train = TRUE, valid = TRUE)
plot(fmnist_dl_regularized, metric = "classification_error")
h2o.scoreHistory(fmnist_dl_regularized)
```


```{r deep-net}
fmnist_dl_deep <- h2o.deeplearning(
    x = 2:785,
    y = "label",
    training_frame = fmnist_train,
    validation_frame = fmnist_holdout,
    model_id = "fmnist_dl_deep",
    hidden = c(256, 128, 64, 32),
    mini_batch_size = 20,
    activation = "RectifierWithDropout",
    hidden_dropout_ratios = c(0.2, 0.2, 0.2, 0.2),
    epochs = 300,
    score_each_iteration = TRUE,
    seed = my_seed
)
h2o.mean_per_class_error(fmnist_dl_deep, train = TRUE, valid = TRUE)
plot(fmnist_dl_deep, metric = "classification_error")
h2o.scoreHistory(fmnist_dl_deep)
```

## Tricks with keras

Keras is a high-level neural network library that runs on top of TensorFlow, an open-sourced end-to-end platform for multiple machine learning tasks developed by Google. Both provide high-level APIs used for easily building and training models, but Keras is more user-friendly.

Keras also has an R interface that we are going to use. You can install it by `install.packages("keras")` and then issuing `install_keras()` and accepting the installation of Miniconda (minimal python env). If you want to use it with virtual environments or custom python installations, follow [these instructions](https://tensorflow.rstudio.com/installation/custom/).

```{r load-mnist-into-keras}
library(keras)
# fmnist_keras <- dataset_fashion_mnist()
# str(fmnist_keras)
```


### A fully connected network example

Similar to what we saw with `h2o`. Keras need some restructure:

```{r adjust-data}
# the transformation takes a while... you can load the same data that is built-in into the keras lib by calling dataset_fashion_mnist()
tbl_fmnist_train <- as_tibble(fmnist_train)
tbl_fmnist_holdout <- as_tibble(fmnist_holdout)

# Separate x & rescale
data_train_x <- as.matrix(select(tbl_fmnist_train, -label))
data_valid_x <- as.matrix(select(tbl_fmnist_holdout, -label))

# Separate y & one-hot encoding
data_train_y <- to_categorical(tbl_fmnist_train$label, 10)
data_valid_y <- to_categorical(tbl_fmnist_holdout$label, 10)
```

```{r keras-model}
simple_keras <- keras_model_sequential()
simple_keras |>
    layer_dense(units = 32, activation = 'relu', input_shape = c(784)) |>
    layer_dropout(rate = 0.2) |>
    layer_dense(units = 10, activation = 'softmax')
```

```{r keras-model-summary}
summary(simple_keras)
# 25,120 = 784 (input features) * 32 (first layer nodes) + 32 (biases)
# 330 = 10 (output nodes) * 32 (first layer) + 10 (biases)
```

```{r keras-model-optimization-setting}
# Loss is optimized during the training, performance is evaluated based on the metric
# The metric itself is not necessarily smooth so it might be not a good idea to directly optimize for that
# compile modifies the model in place
compile(
    simple_keras,
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
)
```

```{r keras-model-training}
fit(
    simple_keras, data_train_x, data_train_y,
    epochs = 30, batch_size = 20,
    validation_data = list(data_valid_x, data_valid_y)
)
```

```{r keras-model-evaluation}
evaluate(simple_keras, data_valid_x, data_valid_y)
```

```{r keras-predictions}
keras_predictions <- predict(simple_keras, data_valid_x)
predicted_labels <- k_argmax(keras_predictions) |> as.numeric()
table(tbl_fmnist_holdout$label, predicted_labels)
```


### A convolutional neural net example

It makes use of the 2d structure of the original input data, applying
filters exploiting the 2d images. In `h2o` there is no option to use such models
by default.

```{r reshape-for-conv}
data_train_2d_x <- array_reshape(data_train_x, c(nrow(data_train_x), 28, 28, 1))
data_valid_2d_x <- array_reshape(data_valid_x, c(nrow(data_valid_x), 28, 28, 1))
```

```{r cnn-definition}
cnn_model <- keras_model_sequential()
cnn_model |> 
    layer_conv_2d(
        filters = 32,
        kernel_size = c(3, 3),
        activation = 'relu',
        input_shape = c(28, 28, 1)
    ) |> 
    layer_max_pooling_2d(pool_size = c(2, 2)) |> 
    layer_dropout(rate = 0.2) |> 
    layer_flatten() |> 
    layer_dense(units = 16, activation = 'relu') |> 
    layer_dense(units = 10, activation = 'softmax')
```

```{r cnn-summary}
summary(cnn_model)
```

Number of parameters:
- `layer_conv_2d` turns 28 x 28 to 26 x 26, using 9 parameters for each filter (3 x 3 weights), plus a bias for each filter, altogether 320 parameters
- `max_pooling2d` takes each disjoint 2 x 2 squares and collapses them to 1, turning a 26 x 26
"image" to a 13 x 13. No parameters are associated with this step.
- `flatten`: turns each "pixel" in each node to one separate node: 13 x 13 x 32 = 5408
- `dense`: fully connected layer: 5408 nodes x 16 new nodes + 16 biases = 86544
- final fully connected layer: 16 x 10 + 10 = 170


```{r cnn-setup}
compile(
    cnn_model,
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c('accuracy')
)
```

```{r cnn-train}
# Larger batch size decreases the training time
fit(
    cnn_model, data_train_2d_x, data_train_y,
    epochs = 30, batch_size = 64,
    validation_data = list(data_valid_2d_x, data_valid_y)
)
```
```{r cnn-evaluation}
evaluate(cnn_model, data_valid_2d_x, data_valid_y)
```




### Data augmentation

You can increase your training sample size and sharpen your model with slightly
modifying your training sample data points, retaining the labels.

Set up steps with which we can alter images a bit:
```{r data-augmentation}
batch_size <- 128

train_datagen <- image_data_generator(
    rotation_range = 20,
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    shear_range = 0.1,
    zoom_range = 0.1
)

valid_datagen <- image_data_generator()

train_generator <- flow_images_from_data(
    x = data_train_2d_x,
    y = data_train_y,
    generator = train_datagen,
    batch_size = batch_size
)

valid_generator <- flow_images_from_data(
    x = data_valid_2d_x,
    y = data_valid_y,
    generator = valid_datagen,
    batch_size = batch_size
)
```

```{r cnn-with-augmentation}
cnn_model_w_augmentation <- keras_model_sequential()
cnn_model_w_augmentation |> 
    layer_conv_2d(
        filters = 32,
        kernel_size = c(3, 3),
        activation = 'relu',
        input_shape = c(28, 28, 1)
    ) |> 
    layer_max_pooling_2d(pool_size = c(2, 2)) |> 
    layer_dropout(rate = 0.2) |> 
    layer_flatten() |> 
    layer_dense(units = 32, activation = 'relu') |> 
    layer_dense(units = 10, activation = 'softmax')

compile(
    cnn_model_w_augmentation,
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c('accuracy')
)

fit(
    cnn_model_w_augmentation,
    train_generator,
    epochs = 50,
    steps_per_epoch = nrow(data_train_x) / batch_size,  # this does not make a difference here -- batch_size of the generator determines how training works
    validation_data = valid_generator,
    validation_steps = nrow(data_valid_x) / batch_size
)
```

```{r}
evaluate(cnn_model_w_augmentation, data_valid_2d_x, data_valid_y)
```



### Transfer learning

If your problem is similar to another one, you might use an already trained model (like the models trained on the famous IMAGENET dataset) to spare time. General patterns in images are common and this knowledge can be "transferred".

Here, I just illustrate how transfer learning works. If you face a more general image recognition problem (e.g. dog vs cat) with less samples, transfer learning might help more.

```{r reshape-for-imagenet}
reshapeForTransferLearning <- function(x) {
    x3D <- replicate(3, array_reshape(x, c(nrow(x), 28, 28)))
    resized_x <- lapply(seq(nrow(x3D)), function(i) image_array_resize(x3D[i,,,], 32, 32))
    do.call(abind::abind, list(resized_x, along = 0))
}

data_train_x_reshaped <- reshapeForTransferLearning(data_train_2d_x)
dim(data_train_x_reshaped)
data_valid_x_reshaped <- reshapeForTransferLearning(data_valid_2d_x)
```

```{r use-imagenet-model}
imagenet_model <- application_densenet121(
    input_shape = c(32, 32, 3), weights = 'imagenet', include_top = FALSE
)
```

```{r transfer-model-setup}
transfer_model <- keras_model_sequential() |> 
    imagenet_model |> 
    layer_flatten() |> 
    layer_dense(units = 128, activation = 'relu') |> 
    layer_dense(units = 10, activation = 'softmax')

compile(
    transfer_model,
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
)
summary(transfer_model)
```

```{r freeze-params}
freeze_weights(imagenet_model, to = "conv5_block16_2_conv")
summary(transfer_model)
```

```{r train-tranfer-model}
fit(
    transfer_model, data_train_x_reshaped, data_train_y,
    epochs = 10, batch_size = 128,
    validation_data = list(data_valid_x_reshaped, data_valid_y)
)
```


